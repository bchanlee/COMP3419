{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jm1S8LvMY3EL",
        "outputId": "a7628d18-bcfe-4559-c589-9bdc639387d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "TDUxcDpFTm5G"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhLAgEliTm5L"
      },
      "source": [
        "### 5.2 Intensity based Object Tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQ51or8vTm5N",
        "outputId": "dc8acdad-095a-4b01-f6df-4441e4ed0ad0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting: 119\n"
          ]
        }
      ],
      "source": [
        "# Setup for object tracking\n",
        "\n",
        "if not os.path.isdir(os.path.join(os.getcwd(), 'frames')):\n",
        "    os.mkdir(\"frames\")\n",
        "else:\n",
        "    print('frames already exists')\n",
        "\n",
        "if not os.path.isdir(os.path.join(os.getcwd(), 'composite')):\n",
        "    os.mkdir(\"composite\")\n",
        "else:\n",
        "    print('composite already exists')\n",
        "    \n",
        "framenumber = 0\n",
        "framectr = 0\n",
        "movie_path = '/content/drive/MyDrive/Colab Notebooks/W5LabData/ping_pang.mov'\n",
        "omovie = cv2.VideoCapture(movie_path)\n",
        "frame_height = omovie.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
        "frame_width = omovie.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
        "\n",
        "# Extract the frames from original video\n",
        "while(1):\n",
        "    ret, frame = omovie.read()\n",
        "    if not ret:\n",
        "        break\n",
        "    print('Extracting: %d' % framenumber)\n",
        "    clear_output(wait=True)\n",
        "    cv2.imwrite('frames/%d.tif' % framenumber, frame)\n",
        "    framenumber += 1\n",
        "omovie.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "gxsZO0WpTm5V"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "    Find the object coordinate by averaging the foreground coordinates\n",
        "    - Function Input: \n",
        "            -         frame:   numpy array, the frame to be processed\n",
        "            -     threshold:           int, the threshold to segment the object\n",
        "\n",
        "    - Function Output:\n",
        "            - [object_x, object_y]: the coordinate of the object in the frame\n",
        "'''\n",
        "def findObj(frame, threshold):\n",
        "    N = 20\n",
        "    object_x_list = []\n",
        "    object_y_list = []\n",
        "\n",
        "    for y in range(frame.shape[1]):\n",
        "      for x in range(frame.shape[0]):\n",
        "        if frame[x][y][0] > threshold and frame[x][y][1] > threshold and frame[x][y][2] > threshold:\n",
        "          object_x_list.append(x)\n",
        "          object_y_list.append(y)\n",
        "\n",
        "    if len(object_x_list) >= N:\n",
        "      object_x = int(np.mean(object_x_list))\n",
        "      object_y = int(np.mean(object_y_list))\n",
        "      return (object_x,object_y)\n",
        "    \n",
        "    return (0,0)\n",
        "\n",
        "\n",
        "# Draw a circle on the image\n",
        "def drawbox(frame, centerx, centery, radius, color):\n",
        "    for y in range(centerx - radius, centerx + radius):\n",
        "        for x in range(centery - radius, centery + radius):\n",
        "            cx = 0 if x < 0 else frame.shape[0]-1 if x > frame.shape[0] - 1 else x\n",
        "            cy = 0 if y < 0 else frame.shape[1]-1 if y > frame.shape[1] - 1 else y\n",
        "            for i in range(3):\n",
        "                frame[cx][cy][i] = color[i]\n",
        "    return frame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iw8XqUsVTm5W"
      },
      "source": [
        "#### Implementation Tips:\n",
        "* You can use np.hstack((img1, img2)) to horizontally stack two images into one\n",
        "* You can use 'cv2.putText(combined_img, text='Sample Text', org=(2200, 1000),fontFace=cv2.FONT_HERSHEY_DUPLEX, fontScale=5, thickness = 5, color=(0, 255, 0)' to put text on a frame\n",
        "* You can use 'cv2.line(img, (x1, y1), (x2, y2), (0, 0, 255), 2)' to draw a red line between two points (x1, y1) and (x2, y2) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0steKE0Tm5X",
        "outputId": "18825c51-3cab-40a0-bf33-b7780dc673a7",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing frame: 81, overall progress: 100.00 %\n"
          ]
        }
      ],
      "source": [
        "framectr = framenumber - 1\n",
        "process_frame = 0\n",
        "\n",
        "foreground = 250 # Foreground Threshold for Segmentation\n",
        "# Store the coordinates found by intensity thresholding\n",
        "coordListX = list()\n",
        "coordListY = list()\n",
        "\n",
        "while process_frame <= framectr:\n",
        "    oframe = cv2.imread('frames/%d.tif' % process_frame)\n",
        "    print('Processing frame: %d, overall progress: %.2f %%' % (process_frame, process_frame/framectr*100))\n",
        "    clear_output(wait=True)\n",
        "    \n",
        "    # Change frame to grey scale\n",
        "    gframe = oframe.copy() # Grey scaled frame\n",
        "\n",
        "    # Load the saved frames sequentially\n",
        "    height = None\n",
        "    width = None\n",
        "    for y in range(gframe.shape[1]):\n",
        "        for x in range(gframe.shape[0]):\n",
        "            # Convert to gray scale\n",
        "            g = 0.212671 * gframe[x][y][2] + 0.715160 * gframe[x][y][1] + 0.072169 * gframe[x][y][0]\n",
        "\n",
        "            # Convert to binary\n",
        "            for i in range(3):\n",
        "                if g > foreground:\n",
        "                    gframe[x][y][i] = 255\n",
        "                else:\n",
        "                    gframe[x][y][i] = 0\n",
        "\n",
        "    # Get the initial state (object coordinates) from binary segmentation\n",
        "    coord = findObj(gframe, 128) # coord is the centre of mass\n",
        "                                 # coord[0] : y column\n",
        "                                 # coord[1] : x row\n",
        "            \n",
        "    # Draw a red dot in the centre of the segmented object\n",
        "    oframe = drawbox(oframe, int(coord[1]), int(coord[0]), 5, (0, 0, 255))\n",
        "    gframe = drawbox(gframe, int(coord[1]), int(coord[0]), 5, (0, 0, 255))\n",
        "    combined_img = np.hstack((oframe, gframe))\n",
        "    ####################################### TODO ###############################################\n",
        "    # 1. You need to perform the object segmentation results on 'gframe'                       #\n",
        "    # 2. You need to display the object moving trajectory on 'oframe'                          #\n",
        "    # 3. You need to display a text to determine whether the object is found or not in a frame #\n",
        "    # 4. Please see the lab sheet for reference                                                #\n",
        "    is_obj_found = False\n",
        "    if coord[0] > 0 and coord[1] > 0:\n",
        "      is_obj_found = True\n",
        "    \n",
        "    if is_obj_found:\n",
        "      text = 'Object found!'\n",
        "      coordListX.append(coord[1])\n",
        "      coordListY.append(coord[0])\n",
        "    if not is_obj_found:\n",
        "      text = 'Object is missing!'\n",
        "\n",
        "    # 2.\n",
        "    for i in range(1, len(coordListX)):\n",
        "      x1 = coordListX[i-1]\n",
        "      y1 = coordListY[i-1]\n",
        "      x2 = coordListX[i]\n",
        "      y2 = coordListY[i]\n",
        "      cv2.line(combined_img, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
        "    # 3.\n",
        "    cv2.putText(combined_img, text=text, org=(2200, 1000),\n",
        "                fontFace=cv2.FONT_HERSHEY_DUPLEX, fontScale=5, thickness = 5, \n",
        "                color=(0, 255, 0))\n",
        "    \n",
        "    ####################################### END ################################################\n",
        "    cv2.imwrite('composite/composite%d.tif' % process_frame, combined_img)\n",
        "    if cv2.waitKey(30) & 0xff == ord('q'):\n",
        "        break\n",
        "    process_frame += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yl9LNSc1Tm5e",
        "outputId": "ea714d54-7275-4595-a6c4-7679176fa7b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video: 68%\n",
            "No more frames to be loaded\n"
          ]
        }
      ],
      "source": [
        "count = 0\n",
        "\n",
        "out = cv2.VideoWriter('./happy_ping_pang.mov', cv2.VideoWriter_fourcc('M', 'J', 'P', 'G'), 10, (int(frame_width*2), int(frame_height)))\n",
        "while(1):\n",
        "    img = cv2.imread('composite/composite%d.tif' % count)\n",
        "    if img is None:\n",
        "        print('No more frames to be loaded')\n",
        "        break\n",
        "    clear_output(wait=True)\n",
        "    out.write(img)\n",
        "    count += 1\n",
        "    print('Saving video: %d%%' % int(100*count/framenumber))\n",
        "    \n",
        "out.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S38eHBaqTm5j"
      },
      "source": [
        "### Intensity based Object Tracking - Week5 Lab Exercise Submission\n",
        "You can now use the generate_results() function below to generate your outputs for submission. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "8RcMOszeTm5k"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "    USYD CODE CITATION ACKNOWLEDGEMENT:\n",
        "    generate_results function is a helper function for you to generate\n",
        "    the output images of lab exercise submission\n",
        "    - Function Input: \n",
        "            -            wk:           int, indicates a specific week's lab exercise\n",
        "            -          name:           str, the name of the student\n",
        "            -           SID:           int, the SID of the student\n",
        "            -  output_video:           str, the path to output_video\n",
        "\n",
        "    - Function Usage:\n",
        "            - Supply all the arguments with the correct types and a result image\n",
        "              will be generated.\n",
        "    - Tips:\n",
        "            - You can right click the result image plot to save the image or \n",
        "              you can take a screenshoot for the submission.\n",
        "'''\n",
        "def generate_results(wk, name, SID, output_video):\n",
        "    cap = cv2.VideoCapture(output_video)\n",
        "    random_frames = []\n",
        "    if not cap.isOpened():\n",
        "        print('%s not opened' % output_video.split('/')[-1])\n",
        "        sys.exit(1)\n",
        "\n",
        "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    x = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
        "    y = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
        "    while x > 10:\n",
        "        x /= (x / 10)\n",
        "        y /= (y / 10)\n",
        "    \n",
        "    random_frames.append(random.randint(53, 63))\n",
        "    random_frames.append(random.randint(66, 76))\n",
        "    random_frames.append(random.randint(80, 94))\n",
        "    \n",
        "    fig, axs = plt.subplots(3, 1, figsize=(x,y))\n",
        "        \n",
        "    count = 0 # 09\n",
        "    output_count = 0\n",
        "    while(1):\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        if count in random_frames:\n",
        "            frame_m = frame.copy()\n",
        "            frame_R = frame[:,:,2]\n",
        "            frame_B = frame[:,:,0]\n",
        "            frame_m[:,:,2] = frame_B\n",
        "            frame_m[:,:,0] = frame_R\n",
        "            frame = np.uint8(frame_m)\n",
        "\n",
        "            axs[output_count].imshow(frame_m)\n",
        "            axs[output_count].text(0.5,-0.1, 'Composite frame: ' + str(count), size=12, ha=\"center\", transform=axs[output_count].transAxes)\n",
        "            axs[output_count].axis('off')\n",
        "            output_count+=1\n",
        "            \n",
        "            if output_count >= 3:\n",
        "                break\n",
        "        count+=1\n",
        "\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    fig.suptitle(\"Week %i Lab Exercise\\n %s SID:%i\"%(wk, name, SID),x=0.5,y=1.05)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ys-eCVttTm5m"
      },
      "outputs": [],
      "source": [
        "# Change the 'path_to_output' to the path where your composited video is located\n",
        "path_to_output = './happy_ping_pang.mov'\n",
        "generate_results(5, 'Name', 'SID', path_to_output)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "COMP3419-W05-Lab-Image Video Processing 2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
